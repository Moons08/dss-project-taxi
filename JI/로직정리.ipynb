{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형회귀 전체적인 개념, 흐름 파악하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개인적으로 선형회귀가 전반적으로 어떤 로직으로 구성이 되고 어떤 개념을 가지고 이 검정을 하고 결과를 해석해야 하는지를 정리하고 싶어서 만들었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 선형회귀 (Linear regression) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_D x_D = w_0 + w^Tx\\;\\; (모수 = w_0, \\cdots, w_D) $$ \n",
    "\n",
    "\n",
    "- 선형회귀는 일반적으로 예측문제에 사용된다. 각각의 독립변수(X)들을 통해 종속변수(y)를 예측하는 것이다. \n",
    "\n",
    "\n",
    "- 일반적으로 결정론적 모형과 확률론적 모형이 있는데 어짜피 우리는 확률론적 모형을 쓰기 때문에 결정론적은 일단 skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 확률론적 선형회귀모형  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률론적 선형회귀모형은 데이터가 `확률 변수로부터 생성된 표본이라고 가정`하는 것이다.\n",
    "\n",
    "\n",
    "- 그렇다면 그 가정엔 무엇이 있을까?\n",
    "\n",
    "\n",
    "    - 선형정규분포의 가정 : 종속 변수  y가 독립 변수  x의 선형 조합으로 결정되는 기댓값과 고정된 분산  σ2을 가지는 정규 분포라는 것이다.\n",
    "    - 오차의 기대값은 x와 상관없이 항상 0\n",
    "    - 각 오차들의 공분산 값은 x와 상관없이 항상 0\n",
    "    - 독립변수의 공분산 행렬은 Full rank\n",
    "    \n",
    "\n",
    "- 또한 우리가 찾아야 하는것은 선형회귀 식에서 w를 찾는 것인데, 이를 찾는 방법이 확률론적에선 MLE (Maximum Likelihood Estimation)이다. MLE는 결과적으론 OLS와 같은 값을 얻을 수 있다. \n",
    "\n",
    "\n",
    "- 오차의 분포가 정규분포를 따르기 때문에 잔차의 분포 또한 정규분포를 따르게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. T 검정 (Sing coefficient T-test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- T검정은 일반적으로 모수가 하나인 경우 활용할 수 있는 것이다. \n",
    "\n",
    "\n",
    "- T검정을 하는 이유는 우리가 구해야 할 w의 값이 0인지 아닌지를 판단하기 위해서다. 0이면 그 변수자체가 의미가 없어진다. \n",
    "\n",
    "\n",
    "| |coef|std err|t| P>t|0.025|0.975|\n",
    "|-|-|\n",
    "|const|-1.6284|2.163|-0.753|0.453|-5.920|2.663|\n",
    "|X1|42.8534|2.142|20.008|0.000|38.603|47.104|\n",
    "\n",
    "\n",
    "- 검정에서 중요한것은 귀무가설을 채택하느냐 안하느냐이다. 일반적으로 p-value가 0.05보다 낮으면 귀무가설 기각, 높으면 채택을 한다. 귀무가설은 `=`으로 표시한다. 즉 T검정에서 귀무가설(H0)는 w = 0 이다.\n",
    "\n",
    "\n",
    "- 여기서 보면 const의 p-value가 0.453으로 0.05보다 큰 값으로 한다. 즉 const의 w=0이라는 말이 성립하고 그 말은 const 값이 0이 되버리기 때문에 의미가 없는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. F 검정 (Loss-of-Fit 검정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F검정은 많은 독립변수들이 있을 때 활용하는 검정이다.\n",
    "\n",
    "\n",
    "$$ H_0 : w_0  = w_1 = \\cdots = w_{K-1} = 0 $$\n",
    "\n",
    "\n",
    "- 식이 의미하는 것은 귀무가설이 모든 w가 0이 된다는 의미다. \n",
    "\n",
    "\n",
    "- 즉 식 자체가 0이 된다는 것인데 현실적으로 이는 거의 불가능하다고 볼 수 있다. 대부분은 기각된다. 즉 w값은 0이 아니라는 뜻이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 레버리지 (Leverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 레버리지는 실제 y가 예측치 yhat에 미치는 영향을 나타낸 값이다.\n",
    "\n",
    "\n",
    "- 일반적으로 생각했을 때, yhat은 y에 가까워야 예측을 잘했다고 할 수 있는 것이고 이는 차이가 별로 없는 것이니까 두개가 유사할 수록 레버리지는 낮아진다고 볼 수 있다.\n",
    "\n",
    "\n",
    "- 반면에 y와 yhat의 차이가 크게 발생하면 레버리지가 큰 것이고, 이는 예측이 잘 안된 경우라고 할 수 있다.\n",
    "\n",
    "\n",
    "- 레버리지가 큰 모델은 값이 왜곡될 가능성이 높아진다. 그 이유는 하나의 값이 그 수준에서 대표값이 되기 때문이다\n",
    "\n",
    "\n",
    "- 따라서 레버리지가 큰 값은 제거해 주는게 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 아웃라이어 (Outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아웃라이어는 잔차가 큰 데이터를 의미한다.\n",
    "\n",
    "\n",
    "- 잔차가 크다는 것은 모형에서 설명하고 있는 데이터와 동떨어진 값을 가지는 데이터라는 뜻이다. \n",
    "\n",
    "\n",
    "- 따라서 아웃라이어는 제거해주는 것이 좋은데 아웃라이어를 제거하는 대표적인 방법이 Cook's Distance이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 분산분석 (ANONA, Analysis of Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 레버리지와 아웃라이어를 제거했으면 선형회귀분석의 결과가 얼마나 좋은지 확인해봐야 한다.\n",
    "\n",
    "\n",
    "- 하지만 단순히 RSS로는 스케일이 변하면 값도 변하기 때문에 이를 보완할 필요성이 있다.\n",
    "\n",
    "\n",
    "- 이를 보완한 방법이 분산분석이고 분산분석은 종속변수의 분산과 독립변수의 분산의 관계를 사용하여 선형회귀분석의 성능을 평가하는 방법이다.\n",
    "\n",
    "\n",
    "- 분산분석에서 봐야 할것은 결정계수($R^2$)이다. 결정계수는 0~1 사이의 값을 나타내며 회귀분석의 성능을 나타낸다\n",
    "\n",
    "\n",
    "- 분산분석과 F-검정은 비슷한 성격을 띄는데 분산분석의 귀무가설과 F-검정의 귀무가설이 사실상 같은 말이기 때문이다. \n",
    "\n",
    "\n",
    "- 모든변수의 w가 0이라는 것은 모형의 의미가 없다는 것이고 이는 $R^2$=0을 의미한다.\n",
    "\n",
    "\n",
    "|\t|df|\tsum_sq|\tmean_sq|\tF|\tPR(>F)|\n",
    "|-|-|\n",
    "|X|\t1.0|\t188589.613492|\t188589.613492|\t179.863766|\t6.601482e-24|\n",
    "|Residual\t|98.0\t|102754.337551\t|1048.513648\t|NaN\t|NaN|\n",
    "\n",
    "\n",
    "- F값을 통해 나온 p-value가 굉장히 작은 값임을 알 수 있다. \n",
    "\n",
    "\n",
    "- 이는 F의 귀무가설이 기각된다는 뜻이고 w이 아니라는 뜻이기 때문에 $R^2$도 0이 아니라는 의미가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.647\n",
      "Model:                            OLS   Adj. R-squared:                  0.644\n",
      "Method:                 Least Squares   F-statistic:                     179.9\n",
      "Date:                Sat, 10 Mar 2018   Prob (F-statistic):           6.60e-24\n",
      "Time:                        23:51:19   Log-Likelihood:                -488.64\n",
      "No. Observations:                 100   AIC:                             981.3\n",
      "Df Residuals:                      98   BIC:                             986.5\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -2.4425      3.244     -0.753      0.453      -8.880       3.995\n",
      "X             43.0873      3.213     13.411      0.000      36.712      49.463\n",
      "==============================================================================\n",
      "Omnibus:                        3.523   Durbin-Watson:                   1.984\n",
      "Prob(Omnibus):                  0.172   Jarque-Bera (JB):                2.059\n",
      "Skew:                          -0.073   Prob(JB):                        0.357\n",
      "Kurtosis:                       2.312   Cond. No.                         1.06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X0, y, coef = make_regression(n_samples=100, n_features=1, noise=30, coef=True, random_state=0)\n",
    "dfX0 = pd.DataFrame(X0, columns=[\"X\"])\n",
    "dfX = sm.add_constant(dfX0)\n",
    "dfy = pd.DataFrame(y, columns=[\"Y\"])\n",
    "df = pd.concat([dfX, dfy], axis=1)\n",
    "\n",
    "model = sm.OLS.from_formula(\"Y ~ X\", data=df)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- R - squared : 결정계수 (높을수록 성능이 좋은 모델)\n",
    "\n",
    "\n",
    "- adj. R-squared : 독립변수 갯수에 따른 효과를 막기 위해 만든 결정개수\n",
    "\n",
    "\n",
    "- AIC, BIC : 정보량 규준 (계산방법이 두개가 다름), 작을수록 좋은 모델\n",
    "\n",
    "\n",
    "- Omnibus, Durbin-Watson : 잔차 정규성 확인 (귀무가설: 잔차 = 0)\n",
    "\n",
    "\n",
    "- Cond.No : 큰 고유치와 가장 작은 고유치의 비율, 작을수록 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 회귀분석 결과 진단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 레버리지, 아웃라이어 제거하고 분산분석을 통해 모델이 유의미하다는 것을 확인했다.\n",
    "\n",
    "\n",
    "- 이걸로 모델이 완성되는걸까? 아니다. 앞에서 언급한 확률론적 모형의 조건도 만족해야 한다.\n",
    "\n",
    "\n",
    "- 제일 많이 확인하는것이 우선 잔차가 정규성을 띄고 있는지에 관한 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 과정을 거친 후엔 다양한 방법들을 조합하여 최적의 모델링을 찾아야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 스케일링 (Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과 진단을 했는데 조건에 맞지 않으면 어떻게 해야할까? 당연히 조건에 맞게끔 조정을 해줘야 한다. \n",
    "\n",
    "\n",
    "- 스케일링은 평균과 표준편차를 이용하여 변수들을 최적화 시키는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 변수 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스케일을 해도 문제가 생길 수 있다. 만약 문제중 다음과 같은 문제가 있으면 변수변환이 하나의 방법이 된다. \n",
    "\n",
    "\n",
    "    - 독립 변수나 종속 변수가 심하게 한쪽으로 치우친 분포를 보이는 경우\n",
    "    - 독립 변수와 종속 변수간의 관계가 곱셈 혹은 나눗셉으로 연결된 경우\n",
    "    - 종속 변수와 예측치가 비선형 관계를 보이는 경우\n",
    "\n",
    "\n",
    "- 변수변환은 데이터에 로그나 제곱등을 취해서 변수를 변환하는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 다중공선성 (Multicollinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다중공선성은 독립변수끼리 뭔가 비슷한게 있어서 최적의 모델링을 방해한다는 의미다. (Full rank가 안됨)\n",
    "\n",
    "\n",
    "- 그래서 변수간 상관관계가 높으면 다중공선성을 의심해봐야 한다.\n",
    "\n",
    "\n",
    "- 다중공선성을 제거하는 방법은 VIF(Variance Inflation Facotor)를 이용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 다항회귀 (Polynomial regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다항회귀는 `선형 기저함수 모형`을 이용하여 모델을 최적화하는 것을 의미한다.\n",
    "\n",
    "\n",
    "- 다항회귀를 하는 이유는 모델의 예측력을 높이기 위함이다. \n",
    "\n",
    "\n",
    "- 다항회귀를 만드는 방법은 변수에 제곱이나 세제곱등을 더해 변수의 개수를 높이면서 만든다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 과최적화 (Overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과최적화는 위의 다항회귀에서 변수들을 너무 많이 모델에 집어넣어서 과도하게 최적화되는 경우다\n",
    "\n",
    "\n",
    "- 모델에 최적화되면 좋은게 아닌가라는 생각을 할 수 있다. 하지만 이는 올바른 모델이 아니다. \n",
    "\n",
    "\n",
    "- 원래 모델의 목적은 회귀식을 통해 지금 가지고 있는 데이터 뿐 아니라 외부 데이터를 가져와도 유의미한 예측을 하는 것이다.\n",
    "\n",
    "\n",
    "- 하지만 오버피팅은 지금 가지고 있는 데이터에만 최적화가 되기 때문에 외부데이터에 대한 결과물을 얻었을때 값이 제대로 안나오게 된다.\n",
    "\n",
    "\n",
    "- 따라서 모델링을 한 후에 오버피팅 여부를 확인하고 오버피팅일시엔 변수를 제거하는 등 조치를 취해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 교차 검증 (Cross - Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교차 검증은 위에서 언급한 오버피팅인지 아닌지의 여부를 확인하는 것을 의미한다.\n",
    "\n",
    "\n",
    "- 오버피팅인지 아닌지를 알기 위해선 Train데이터와 Test데이터가 있어야 Train에서 적용한 모델링을 Test에 적용하고 이들이 결과값이 비슷하게 나와야 오버피팅이 아닐 것이다.\n",
    "\n",
    "\n",
    "- 현실적으로는 외부에서 데이터를 가져오는게 힘드니까 있는 데이터를 쪼개서 Train과 Test데이터를 만들곤 한다. 그럼 쪼개는 방법에는 뭐가 있을까?\n",
    "\n",
    "\n",
    "    - K-fold (전체 데이터를 K개로 쪼개서 K중 하나만 Test, 하나를 뺀 K-1개는 Train데이터로 활용, 이를 K번 실행)\n",
    "    - LeaveOneOut (하나의 Test을 만들고, 그걸 제외한 데이터에서 Train데이터를 만들어 검증) \n",
    "    - ShuffleSplit (데이터를 중복해서 실행)\n",
    "\n",
    "\n",
    "- 세가지 방법이 있다. 이 세가지의 특징은 단순히 한번 쪼개서 검증하는게 아니라 나름대로의 방법으로 여러번 쪼개서 검증하는 것이다. 이렇게 복수개의 Train데이터와 Test데이터를 검증해주기 위해서 `Cross Validataion calculater`를 통해 결과를 보여준다.\n",
    "\n",
    "\n",
    "- 이렇게 해서 결과물이 오버피팅이 나왔으면 우린 어떻게 해야할까? 한가지 방법으로 선형회귀를 정규화 시키는 방법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. 정규화 선형회귀 (Regularized linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규화 선형회귀는 오버피팅된 것을 조절해주기 위해서 만들어 졌다.\n",
    "\n",
    "\n",
    "- 방법은 선형 회귀 계수에 제약조건을 추가해서 오버피팅을 막는 것이다. \n",
    "\n",
    "\n",
    "- 일반적으로 오버피팅이 되면 계수의 크기도 과도하게 증가되는 경향이 있어서 계수의 크기를 제한하여 오버피팅을 막는다. 이 방법에는 3가지가 있다. \n",
    "\n",
    "\n",
    "    - Ridge 회귀모형 : 제곱합을 통해 계수를 제한 (Skit-Learn)\n",
    "    - Lasso 회귀모형 : 절대값을 통해 계수를 제한 (Skit-Learn)\n",
    "    - Elastic Net 회귀모형 : 제곱합 + 절대값을 통해 계수를 제한 (Skit-Learn, StatesModel)\n",
    "\n",
    "\n",
    "- 이러한 방법으로 모델링을 하게 되면 사용되는 데이터가 달라져도 계수가 크게 달라지지 않는다. 즉, Test데이터와 Train데이터의 결과값이 비슷하게 나오게 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. 최적 정규화 (Optimal normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오버피팅을 없앴으면 이제 최적 정규화를 통해 최적의 모델을 만들어야 한다.\n",
    "\n",
    "\n",
    "- 제일 기본적으로 두가지 방법이 있다. 첫번째로는 차수를 바꿔보는 것이다. 다항회귀의 차수를 바꾸면서 오차와 분산을 측정해보면서 최적의 결과를 도출해 내보는 것이 좋다.\n",
    "\n",
    "\n",
    "- 두번째로는 모수의 개수를 바꿔보는 것이다. 성능이 비슷하다면 모수의 개수는 적을수록 좋다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
